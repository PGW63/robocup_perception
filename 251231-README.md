사용방법:
'''
source detection_venv/bin/activate 
source ~/vision_ws/install/setup.bash

ros2 run perception yolo_node
ros2 run perception groundedsam2_node

ros2 run perception groundedsam2_node --ros-args   -p enable_pointcloud:=true   -p filtered_depth_topic:=/detection_node/filtered_depth_image   -p camera_info_topic:=/camera/camera/aligned_depth_to_color/camera_info   -p min_depth:=0.1   -p max_depth:=10.0

'''

venv안에 colcon build tools까지 깔아놨으므로, build 또한 venv를 키고 진행해주어야함.
venv를 안키고 하면 시스템 파이썬으로 빌드되기 때문에 진행이 되지 않는 듯함.

디텍션 실행 방법:

구조:

파라미터:


토픽:
/detection_node/debug_image : 탐지에 성공한 물체들을 시각화할 이미지 토픽
/detection_node/use_segmentation : True -> Grounded-SAM2 or False -> YOLO
/detection_node/search : 'coke. table. pringles.' ->  Grounded-SAM2에서 찾을 물체들
/detection_node/stop : Grounded-SAM2에서 tracking을 멈추게 할 트리거
/detection_node/detections : results에 class_id 및 bbox의 center, bbox의 크기 가 토픽으로 나옴
/detection_node/status



