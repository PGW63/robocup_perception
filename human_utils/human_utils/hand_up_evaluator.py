#!/usr/bin/env python3
"""
ì† ë“¤ê¸° ê°ì§€ í‰ê°€ ë…¸ë“œ (Hand Up Goal Evaluator) - ìˆ˜ì •ë³¸
- ì¢Œí‘œê³„ ë§¤í•‘ (Optical Z -> Robot X, Optical X -> Robot -Y)
- ì„¸ì…˜ë³„ RMSE ê³„ì‚° (GT ì…ë ¥ í›„ 10ì´ˆê°„ì˜ ë°ì´í„°ë§Œ ë§¤ì¹­)
"""

import numpy as np
import cv2
import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy
from sensor_msgs.msg import Image
from std_msgs.msg import String
from visualization_msgs.msg import MarkerArray
import json
from datetime import datetime
from pathlib import Path
import threading
import time


class HandUpEvaluator(Node):
    def __init__(self):
        super().__init__('hand_up_evaluator')
        
        # íŒŒë¼ë¯¸í„° ì„ ì–¸
        self.declare_parameter("image_topic", "/human/debug_image")
        self.declare_parameter("states_topic", "/human/states")
        self.declare_parameter("markers_topic", "/human/skeleton_markers")
        self.declare_parameter("capture_fps", 3)
        self.declare_parameter("eval_dir", "hand_up_eval_results")
        
        self.image_topic = self.get_parameter("image_topic").value
        self.states_topic = self.get_parameter("states_topic").value
        self.markers_topic = self.get_parameter("markers_topic").value
        self.capture_fps = self.get_parameter("capture_fps").value
        self.eval_path = Path(self.get_parameter("eval_dir").value)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.eval_path.mkdir(exist_ok=True)
        self.image_dir = self.eval_path / "images"
        self.image_dir.mkdir(exist_ok=True)
        
        # QoS ì„¤ì •
        qos = QoSProfile(
            reliability=ReliabilityPolicy.BEST_EFFORT,
            history=HistoryPolicy.KEEP_LAST,
            depth=5,
        )
        
        # êµ¬ë…ì
        self.sub_image = self.create_subscription(Image, self.image_topic, self.on_image, qos)
        self.sub_states = self.create_subscription(String, self.states_topic, self.on_states, 10)
        self.sub_markers = self.create_subscription(MarkerArray, self.markers_topic, self.on_markers, qos)
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.evaluation_data = {
            "start_time": datetime.now().isoformat(),
            "trials": [],          # {gt: [x, y], preds: [[x, y], ...]} í˜•íƒœì˜ ì„¸ì…˜ ê¸°ë¡
            "false_positives": 0,
            "frames_captured": 0
        }
        
        # ì‹¤ì‹œê°„ ìƒíƒœ ë³€ìˆ˜
        self.evaluation_active = False
        self.evaluation_start_time = None
        self.evaluation_duration = 10.0
        self.current_gt = None
        self.current_session_preds = []
        self.last_capture_time = 0
        
        # ì…ë ¥ ìŠ¤ë ˆë“œ
        self.input_thread = threading.Thread(target=self._input_thread, daemon=True)
        self.input_thread.start()
        
        self.get_logger().info("âœ… Hand Up Evaluator Ready")
        self.get_logger().info("ëª…ë ¹ì–´: 'gt x y' (ì˜ˆ: gt 3.4 0), 'fp' (ì˜¤íƒì§€), 'save' (ì €ì¥), 'clear' (ì´ˆê¸°í™”)")

    def _input_thread(self):
        while rclpy.ok():
            try:
                user_input = input("\n[HandUpEvaluator] ëª…ë ¹: ").strip()
                if user_input.startswith("gt "):
                    parts = user_input.split()
                    x, y = float(parts[1]), float(parts[2])
                    self.start_evaluation_session(x, y)
                elif user_input == "fp":
                    self.evaluation_data["false_positives"] += 1
                    self.get_logger().info(f"âŒ False Positive ì¹´ìš´íŠ¸: {self.evaluation_data['false_positives']}")
                elif user_input == "save":
                    self.save_results()
                elif user_input == "clear":
                    self.evaluation_data["trials"] = []
                    self.evaluation_data["false_positives"] = 0
                    self.get_logger().info("ğŸ§¹ ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                elif user_input in ["exit", "quit"]:
                    break
            except Exception as e:
                self.get_logger().error(f"ì…ë ¥ ì˜¤ë¥˜: {e}")

    def start_evaluation_session(self, x: float, y: float):
        """GT ì…ë ¥ í›„ 3ì´ˆ ëŒ€ê¸° í›„ 10ì´ˆê°„ ì¸¡ì • ì‹œì‘"""
        self.current_gt = [x, y]
        self.current_session_preds = []
        
        self.get_logger().info(f"â±ï¸ GT({x}, {y}) ì…ë ¥ë¨. 3ì´ˆ ë’¤ ì¸¡ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for i in range(3, 0, -1):
            self.get_logger().info(f">>> {i}...")
            time.sleep(1)
        
        self.evaluation_start_time = time.time()
        self.evaluation_active = True
        self.get_logger().info(f"ğŸš€ ì¸¡ì • ì‹œì‘! (10ì´ˆê°„)")

    def on_markers(self, msg: MarkerArray):
        """ìŠ¤ì¼ˆë ˆí†¤ ë§ˆì»¤ë¡œë¶€í„° ì¢Œí‘œ ì¶”ì¶œ ë° ë§¤í•‘"""
        if not self.evaluation_active or not msg.markers:
            return
        
        elapsed = time.time() - self.evaluation_start_time
        if elapsed > self.evaluation_duration:
            # 10ì´ˆ ì¢…ë£Œ ì‹œ ì„¸ì…˜ ë°ì´í„° ì €ì¥
            self.evaluation_active = False
            self.evaluation_data["trials"].append({
                "gt": self.current_gt,
                "preds": self.current_session_preds,
                "timestamp": datetime.now().isoformat()
            })
            self.get_logger().info(f"â¹ï¸ ì¸¡ì • ì¢…ë£Œ (ìˆ˜ì§‘ëœ ë°ì´í„°: {len(self.current_session_preds)}ê°œ)")
            return

        try:
            joint_marker = msg.markers[0]
            l_idx, r_idx = 5, 6 # ì–´ê¹¨ ì¸ë±ìŠ¤
            if len(joint_marker.points) > r_idx:
                p_l = joint_marker.points[l_idx]
                p_r = joint_marker.points[r_idx]
                
                # Camera Optical Frame ì›ë³¸
                raw_x = (p_l.x + p_r.x) / 2.0
                raw_z = (p_l.z + p_r.z) / 2.0
                
                # [ì¢Œí‘œ ë³€í™˜ ë§¤í•‘]
                # Optical Z(ì •ë©´) -> Robot X
                # Optical X(ì˜¤ë¥¸ìª½+) -> Robot Y(ì™¼ìª½+) ì´ë¯€ë¡œ ë¶€í˜¸ ë°˜ì „
                mapped_x = raw_z
                mapped_y = -raw_x
                
                self.current_session_preds.append([mapped_x, mapped_y])
                
                if len(self.current_session_preds) % 10 == 0:
                    self.get_logger().info(f"ğŸ“ ì‹¤ì‹œê°„ ë§¤í•‘: X:{mapped_x:.2f}, Y:{mapped_y:.2f} (ì˜¤ì°¨: {np.linalg.norm(np.array([mapped_x, mapped_y])-np.array(self.current_gt)):.3f}m)")
        except Exception:
            pass

    def on_image(self, msg: Image):
        """ì´ë¯¸ì§€ ìº¡ì²˜ (3fps)"""
        curr = time.time()
        if curr - self.last_capture_time >= (1.0 / self.capture_fps):
            try:
                bgr = np.frombuffer(msg.data, dtype=np.uint8).reshape((msg.height, msg.width, -1))
                filename = self.image_dir / f"frame_{self.evaluation_data['frames_captured']:05d}.png"
                cv2.imwrite(str(filename), bgr)
                self.evaluation_data["frames_captured"] += 1
                self.last_capture_time = curr
            except Exception:
                pass

    def on_states(self, msg: String):
        pass # í•„ìš” ì‹œ ì¶”ê°€

    def calculate_metrics(self):
        """ëª¨ë“  Trialì— ëŒ€í•œ RMSE ê³„ì‚°"""
        all_errors = []
        if not self.evaluation_data["trials"]:
            return {"rmse": 0.0, "mae": 0.0}
        
        for trial in self.evaluation_data["trials"]:
            gt = np.array(trial["gt"])
            preds = np.array(trial["preds"])
            if len(preds) == 0: continue
            
            # ê° ì˜ˆì¸¡ê°’ê³¼ GT ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
            dists = np.linalg.norm(preds - gt, axis=1)
            all_errors.extend(dists.tolist())
            
        if not all_errors:
            return {"rmse": 0.0, "mae": 0.0}
            
        errors_np = np.array(all_errors)
        rmse = np.sqrt(np.mean(errors_np**2))
        mae = np.mean(errors_np)
        return {"rmse": float(rmse), "mae": float(mae)}

    def save_results(self):
        metrics = self.calculate_metrics()
        final_results = {
            "summary": metrics,
            "false_positives": self.evaluation_data["false_positives"],
            "total_trials": len(self.evaluation_data["trials"]),
            "details": self.evaluation_data["trials"]
        }
        
        filename = self.eval_path / f"eval_{datetime.now().strftime('%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(final_results, f, indent=2)
            
        self.get_logger().info(f"\n" + "="*50)
        self.get_logger().info(f"ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼ ({len(self.evaluation_data['trials'])} ì„¸ì…˜)")
        self.get_logger().info(f"  - ì¢Œí‘œ RMSE: {metrics['rmse']:.4f} m")
        self.get_logger().info(f"  - ì¢Œí‘œ MAE : {metrics['mae']:.4f} m")
        self.get_logger().info(f"  - ê±°ì§“ ì–‘ì„±: {final_results['false_positives']} íšŒ")
        self.get_logger().info(f"  - ê²°ê³¼ ì €ì¥: {filename}")
        self.get_logger().info("="*50 + "\n")


def main(args=None):
    rclpy.init(args=args)
    node = HandUpEvaluator()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == "__main__":
    main()